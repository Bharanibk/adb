trigger:
  branches:
    include:
      - main

pool:
  vmImage: ubuntu-latest

variables:
- group: databricks-stage   # contains databricks-host, databricks-token
- name: SRC_NOTEBOOKS_DIR
  value: notebook           # matches your repo folder name
- name: DEST_WORKSPACE_DIR
  value: /Workspace/notebook   # where you want them to appear in Stage workspace

stages:
- stage: Deploy_Stage
  displayName: Copy notebooks to Staging
  jobs:
  - job: import_notebooks
    steps:
    - checkout: self
      fetchDepth: 0

    - task: UsePythonVersion@0
      inputs:
        versionSpec: '3.10'

    - script: |
        python -m pip install --upgrade pip
        pip install --upgrade databricks-cli
        databricks --version
      displayName: Install Databricks CLI

    - script: |
        set -e
        export DATABRICKS_HOST="$(databricks-host)"
        export DATABRICKS_TOKEN="$(databricks-token)"

        # Ensure destination exists
        databricks workspace mkdirs "$(DEST_WORKSPACE_DIR)"

        # Import all notebooks recursively and overwrite
        databricks workspace import_dir \
          --overwrite \
          "$(SRC_NOTEBOOKS_DIR)" \
          "$(DEST_WORKSPACE_DIR)"
      displayName: Import notebooks to Stage
