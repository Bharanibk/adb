trigger:
  branches:
    include:
      - main

pool:
  vmImage: ubuntu-latest

# Uses your variable group with: databricks-host, databricks-token
variables:
- group: databricks-stage

# Change these if you want a different source or destination path
- name: SRC_NOTEBOOKS_DIR
  value: notebooks
- name: DEST_WORKSPACE_DIR
  value: /Shared/deploy/notebooks   # will be created if it doesn't exist

stages:
- stage: Deploy_Stage
  displayName: Copy notebooks to Staging workspace
  jobs:
  - job: import_notebooks
    displayName: Import notebooks
    steps:
    - checkout: self
      fetchDepth: 0

    - task: UsePythonVersion@0
      inputs:
        versionSpec: '3.10'

    - script: |
        set -e
        sudo apt-get update -y
        sudo apt-get install -y unzip
        mkdir -p $HOME/.databricks/bin
        curl -L https://github.com/databricks/cli/releases/latest/download/databricks_linux_amd64.zip -o /tmp/databricks_cli.zip
        unzip -o /tmp/databricks_cli.zip -d $HOME/.databricks/bin
        chmod +x $HOME/.databricks/bin/databricks
        echo '##vso[task.prependpath]'$HOME'/.databricks/bin'
        databricks version
      displayName: Install Databricks CLI (download from GitHub Releases)


    - script: |
        # Auth for CLI
        export DATABRICKS_HOST="$(databricks-host)"
        export DATABRICKS_TOKEN="$(databricks-token)"

        # Ensure destination folder exists
        databricks workspace mkdirs "$(DEST_WORKSPACE_DIR)"

        # Import the entire notebooks folder (recursive) and overwrite
        # AUTO format will handle .py and .ipynb
        databricks workspace import-dir \
          --overwrite \
          --format AUTO \
          "$(SRC_NOTEBOOKS_DIR)" \
          "$(DEST_WORKSPACE_DIR)"
      displayName: Import notebooks to workspace
