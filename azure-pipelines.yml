trigger:
  branches:
    include:
      - main

pool:
  vmImage: ubuntu-latest

# Uses your variable group with: databricks-host, databricks-token
variables:
- group: databricks-stage

# Change these if you want a different source or destination path
- name: SRC_NOTEBOOKS_DIR
  value: notebooks
- name: DEST_WORKSPACE_DIR
  value: /Shared/deploy/notebooks   # will be created if it doesn't exist

stages:
- stage: Deploy_Stage
  displayName: Copy notebooks to Staging workspace
  jobs:
  - job: import_notebooks
    displayName: Import notebooks
    steps:
    - checkout: self
      fetchDepth: 0

    - task: UsePythonVersion@0
      inputs:
        versionSpec: '3.10'

    - script: |
        # Install NEW Databricks CLI (Go)
        curl -fsSL https://raw.githubusercontent.com/databricks/cli/main/scripts/install.sh | bash
        echo '##vso[task.prependpath]$HOME/.databricks/bin'
        databricks version
      displayName: Install Databricks CLI (new)

    - script: |
        # Auth for CLI
        export DATABRICKS_HOST="$(databricks-host)"
        export DATABRICKS_TOKEN="$(databricks-token)"

        # Ensure destination folder exists
        databricks workspace mkdirs "$(DEST_WORKSPACE_DIR)"

        # Import the entire notebooks folder (recursive) and overwrite
        # AUTO format will handle .py and .ipynb
        databricks workspace import-dir \
          --overwrite \
          --format AUTO \
          "$(SRC_NOTEBOOKS_DIR)" \
          "$(DEST_WORKSPACE_DIR)"
      displayName: Import notebooks to workspace
