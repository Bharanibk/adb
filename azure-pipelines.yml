trigger:
- main

pool:
  vmImage: ubuntu-latest

variables:
- group: databricks-stage         # has: databricks-host, databricks-token
- name: SRC_NOTEBOOKS_DIR
  value: notebook                 # folder in your repo (adjust if different)
- name: DEST_WORKSPACE_DIR
  value: /Shared/deploy/notebooks # where to copy in STAGING workspace

steps:
- checkout: self
  fetchDepth: 0

- task: UsePythonVersion@0
  inputs:
    versionSpec: '3.10'

- script: |
    python -m pip install --upgrade pip
    pip install --upgrade databricks-cli
    databricks --version
  displayName: Install Databricks CLI (legacy)

- script: |
    set -e
    echo "Using host: $(databricks-host)"
    export DATABRICKS_HOST="$(databricks-host)"
    export DATABRICKS_TOKEN="$(databricks-token)"

    echo "Listing / before import"
    databricks workspace ls /

    echo "Ensuring destination: $(DEST_WORKSPACE_DIR)"
    databricks workspace mkdirs "$(DEST_WORKSPACE_DIR)"

    echo "Importing from repo folder: $(SRC_NOTEBOOKS_DIR)"
    databricks workspace import_dir --overwrite "$(SRC_NOTEBOOKS_DIR)" "$(DEST_WORKSPACE_DIR)"

    echo "Listing destination after import"
    databricks workspace ls "$(DEST_WORKSPACE_DIR)"
  displayName: Import notebooks to STAGING

# Upsert the job in STAGING (reset if exists, else create)
- script: |
    set -e
    export DATABRICKS_HOST="$(databricks-host)"
    export DATABRICKS_TOKEN="$(databricks-token)"

    # Ensure jq is available
    sudo apt-get update -y
    sudo apt-get install -y jq

    JOB_NAME="employee_master_job"
    JOB_JSON="jobs/employee_master_job.stage.json"

    echo "Using host: $DATABRICKS_HOST"
    databricks --version

    echo "Switching Jobs CLI to API 2.1"
    databricks jobs configure --version=2.1

    echo "Looking for existing job named: $JOB_NAME"
    JOB_ID=$(databricks jobs list --output JSON | jq -r '.jobs[] | select(.settings.name=="'"$JOB_NAME"'") | .job_id' | head -n1)

    if [ -n "$JOB_ID" ] && [ "$JOB_ID" != "null" ]; then
      echo "Found job_id=$JOB_ID — resetting with new settings (2.1)"
      databricks jobs reset --job-id "$JOB_ID" --new-settings-file "$JOB_JSON"
      # Some CLI builds don’t support --new-settings-file; if that fails, fallback:
      if [ $? -ne 0 ]; then
        echo "Fallback to --new-settings @file"
        databricks jobs reset --job-id "$JOB_ID" --new-settings "@$JOB_JSON"
      fi
    else
      echo "Job not found — creating new job (2.1)"
      # Prefer the file flag to avoid JSONDecode issues
      databricks jobs create --json-file "$JOB_JSON" || databricks jobs create --json "@$JOB_JSON"
    fi

    echo "Done. Current jobs (names):"
    databricks jobs list --output JSON | jq -r '.jobs[].settings.name'
  displayName: Upsert Databricks job (stage, API 2.1)

