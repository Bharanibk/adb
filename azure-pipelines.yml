trigger:
  branches:
    include:
      - main

pool:
  vmImage: ubuntu-latest

variables:
- group: databricks-stage     # contains: databricks-host, databricks-token
- name: SRC_NOTEBOOKS_DIR
  value: notebooks            # path in your repo
- name: DEST_WORKSPACE_DIR
  value: /Shared/deploy/notebooks   # path in Stage workspace

stages:
- stage: Deploy_Stage
  displayName: Copy notebooks to Staging
  jobs:
  - job: import_notebooks
    steps:
    - checkout: self
      fetchDepth: 0

    - task: UsePythonVersion@0
      inputs:
        versionSpec: '3.10'

    # Install the *legacy* CLI (works great for workspace import_dir)
    - script: |
        python -m pip install --upgrade pip
        pip install --upgrade databricks-cli
        databricks --version
      displayName: Install legacy Databricks CLI

    # Auth + mkdirs + import_dir (legacy syntax uses underscores)
    - script: |
        set -e
        export DATABRICKS_HOST="$(databricks-host)"
        export DATABRICKS_TOKEN="$(databricks-token)"

        # Ensure destination exists
        databricks workspace mkdirs "$(DEST_WORKSPACE_DIR)"

        # Import the entire folder recursively and overwrite
        # NOTE: If your notebooks are .py, set language to PYTHON
        databricks workspace import_dir \
          --overwrite \
          --language PYTHON \
          "$(SRC_NOTEBOOKS_DIR)" \
          "$(DEST_WORKSPACE_DIR)"
      displayName: Import notebooks to Stage (legacy CLI)
